
from PIL import Image
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transform
from torch.utils.data import Dataset, DataLoader
import os
import numpy as np
from efficientnet_pytorch import EfficientNet
import matplotlib.pyplot as plt


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

tfms = transform.Compose([transform.Resize((224,224)), transform.ToTensor(),
    transform.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])

number_epoch = 30

#加载数据集
train_dataset =torchvision.datasets.ImageFolder(root=r'D:\pycham\project\learn_pytorch\Garbage classification\dataset\train',transform=tfms)
train_loader =DataLoader(train_dataset,batch_size=4, shuffle=True)

val_dataset =torchvision.datasets.ImageFolder(root=r'D:\pycham\project\learn_pytorch\Garbage classification\dataset\val',transform=tfms)
val_loader =DataLoader(train_dataset,batch_size=4, shuffle=True)

test_dataset =torchvision.datasets.ImageFolder(root=r'D:\pycham\project\learn_pytorch\Garbage classification\dataset\test',transform=tfms)
test_loader =DataLoader(train_dataset,batch_size=4, shuffle=True)

# Download and load the pretrained ResNet-18.
model = EfficientNet.from_pretrained('efficientnet-b0')

# print(model)

# If you want to finetune only the top layer of the model, set as below.
for param in model.parameters():
    param.requires_grad = False

# Replace the top layer for finetuning.
model._fc = nn.Linear(model._fc.in_features, 6)  # 100 is an example.
model.to(device)
print(model)

critirion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(),lr=0.001)

total_step = len(train_loader)

train_error = []
val_error = []
test_error = []
for epoch in range(number_epoch):

    sum_loss = 0.0
    train_correct = 0.0
    total_train_sample = 0.0

    for i,(image,label) in enumerate(train_loader):

        inputs = image.to(device)
        targets = label.to(device)

        outputs = model(inputs)
        loss = critirion(outputs,targets)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        _, predict = torch.max(outputs, 1)
        train_correct += (predict == targets).sum().item()
        total_train_sample += len(label)

        if (i +1 )%100 == 0:
            print('Epoch:[{}/{}],step:[{}/{}],loss:{:.4f}'.format(epoch+1,number_epoch,i+1,total_step,loss.item()))
    total_epoch_error = ((total_train_sample - train_correct)/ total_train_sample)
    print('train error{}.'.format(total_epoch_error))
    train_error.append(total_epoch_error)

    """
    Use both. They do different things, and have different scopes.
    with torch.no_grad： disables tracking of gradients in autograd.
    model.eval()： changes the forward() behaviour of the module it is called upon.
    eg, it disables dropout and has batch norm use the entire population statistics
    """
#每过一代验证一次
    print('model val')
    model.eval()
    with torch.no_grad():
        correct = 0
        total_sample = 0
        for image, label in val_loader:
            input = image.to(device)
            targets = label.to(device)

            output = model(input)
            _, predict = torch.max(output, 1)
            correct += (predict == targets).sum().item()
            total_sample += len(label)

        print('total val photo:{},error={}'.format(total_sample, (total_sample - correct) / total_sample))
        val_error.append((total_sample, (total_sample - correct) / total_sample))

    #每过一代测试一下
    print('model test')
    model.eval()
    with torch.no_grad():
        correct = 0
        total_sample = 0
        for image, label in test_loader:
            input = image.to(device)
            targets = label.to(device)

            output = model(input)
            _, predict = torch.max(output, 1)
            correct += (predict == targets).sum().item()
            total_sample += len(label)

        print('total test photo:{},error={}'.format(total_sample, (total_sample - correct) / total_sample))
        test_error.append((total_sample, (total_sample - correct) / total_sample))

plt.plot(range(number_epoch),train_error,'r',range(number_epoch),val_error,'g',range(number_epoch),test_error,'b')
plt.show()




